{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apETXSk7ez2Y",
        "outputId": "cb083ae9-62f5-4879-9a80-78a0ec5b27bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from scipy.signal import find_peaks\n",
        "# import sys, re\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import scipy.io.wavfile as wave\n",
        "# from scipy.signal import butter, lfilter\n",
        "# from datetime import datetime\n",
        "# from scipy.fftpack import dct\n",
        "\n",
        "\n",
        "\n",
        "# def plotspectrogramheatmap1( freqarray, magarray, signalsecs, specfreqmin, specfreqmax, specgramdotsize, specheatmaptype, fontsize):\n",
        "\n",
        "# \t# y-axis as scale of the range, number of spectra in spectrogram\n",
        "# \ty = np.linspace(specfreqmin,specfreqmax,len(magarray[0]))\n",
        "\n",
        "# \t# x-axis as signal time range, number of spectra in spectrogram\n",
        "# \tx = np.linspace(0,signalsecs,len(magarray))\n",
        "\n",
        "# \t# Colormap is derived from magnitudes at each frequency/spectrum\n",
        "# \tfor i, freqvals, magvals in zip(x, freqarray, magarray):\n",
        "# \t\tfreqvals = freqvals[1:]\n",
        "# \t\tmagvals = magvals[1:]\n",
        "# \t\tx = [i] * len(freqvals)\n",
        "# \t\tplt.scatter(\n",
        "# \t\t\tx,freqvals, c=magvals, cmap=specheatmaptype,\n",
        "# \t\t\tmarker=\"s\", s=specgramdotsize)\n",
        "\n",
        "# \t# Spectrogram properties\n",
        "# \tplt.xlim(0,np.ceil(signalsecs))\n",
        "# \tsfmin = np.floor(specfreqmin)\n",
        "# \tsfmax = np.ceil(specfreqmax)\n",
        "# \tplt.ylim(sfmin,sfmax)\n",
        "# \tplt.grid(visible=True, which=\"major\", axis=\"both\")\n",
        "# \t# plt.set_xlabel(\"Time (s)\", fontsize=fontsize)\n",
        "# \t# plt.set_ylabel(\"Freq (Hz)\", fontsize=fontsize)\n",
        "#   # plt.show()\n",
        "\n",
        "# \treturn"
      ],
      "metadata": {
        "id": "yRTfki688EUM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "\n",
        "# def load_ammagarray_from_directory(file_path):\n",
        "#   with open(file_path, 'rb') as pkl_file:\n",
        "#     spectrogram_data = pickle.load(pkl_file)\n",
        "#     ammagarray = spectrogram_data['ammagarray']\n",
        "#     amfreqarray = spectrogram_data['amfreqarray']\n",
        "#     ammaxmags = spectrogram_data['ammaxmags']\n",
        "#     ammaxfreqs = spectrogram_data['ammaxfreqs']\n",
        "#     fs = spectrogram_data['fs']\n",
        "#     signal = spectrogram_data['signal']\n",
        "\n",
        "#   return ammagarray,amfreqarray,ammaxmags,ammaxfreqs,fs,signal"
      ],
      "metadata": {
        "id": "pRCDVLxj8Gl0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fontsize = 10\n",
        "# specgramdotsize = 60\n",
        "# specheatmaptype = \"YlOrRd\"\n",
        "# amspecfreqmin = 0\n",
        "# amspecfreqmax = 10\n",
        "\n",
        "# ammagarrays,amfreqarray,ammaxmags,ammaxfreqs,fs,signal = load_ammagarray_from_directory('/content/gdrive/MyDrive/rnd/pkl_files/Test/Test_D/neutral_out_86.pkl')\n",
        "\n",
        "\n",
        "\n",
        "# plotspectrogramheatmap1(amfreqarray, ammagarray, signalseconds, amspecfreqmin, amspecfreqmax, specgramdotsize, specheatmaptype, fontsize)"
      ],
      "metadata": {
        "id": "S_H4oERr8HZO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from skimage import io, color, metrics, transform\n",
        "# import cv2\n",
        "\n",
        "# # Step 1: Define data directories\n",
        "# train_dir = \"/content/gdrive/MyDrive/rnd/pkl_files_with_fs/Training\"  # Contains \"utterances_d\" and \"utterances_nd\" folders\n",
        "# test_dir = \"/content/gdrive/MyDrive/rnd/pkl_files_with_fs/Test\"    # Contains \"test_d\" and \"test_nd\" folders\n",
        "\n",
        "\n",
        "# image_height, image_width = 224,224  # Adjust as needed\n",
        "# num_classes = 2  # Depressed and non-depressed\n",
        "\n",
        "\n",
        "# def load_images_and_labels(data_dir, num_samples_per_category):\n",
        "#     # i=1\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     for label, category in enumerate([\"Test_D\", \"Test_ND\"]):\n",
        "#           category_dir = os.path.join(data_dir, category)\n",
        "#           image_files = [f for f in os.listdir(category_dir) if f.endswith(\".pkl\")]\n",
        "\n",
        "#           # If there are fewer samples in this category, select all available samples\n",
        "#           # num_samples = min(len(image_files), num_samples_per_category)\n",
        "#           selected_files = np.random.choice(image_files, num_samples_per_category, replace=False)\n",
        "\n",
        "#           for image_file in selected_files:\n",
        "#               # if i<3:\n",
        "#                 image_path = os.path.join(category_dir, image_file)\n",
        "\n",
        "#                 ammagarray,amfreqarray,ammaxmags,ammaxfreqs,fs,signal = load_ammagarray_from_directory(image_path)\n",
        "\n",
        "#                 signallength = len(signal)\t\t# define numerical signal length\n",
        "#                 signalseconds = signallength / fs\t# define signal length in seconds\n",
        "#                 signal = signal / max(abs(signal))\n",
        "\n",
        "#                 plotspectrogramheatmap1(amfreqarray, ammagarray, signalseconds, amspecfreqmin, amspecfreqmax, specgramdotsize, specheatmaptype, fontsize)\n",
        "\n",
        "#                 fig=plt.gcf()\n",
        "#                 fig.canvas.draw()\n",
        "#                 plot_data_rgba = np.array(fig.canvas.renderer.buffer_rgba())\n",
        "#                 plt.close()\n",
        "#                 plot_data_rgb = plot_data_rgba[:, :, :3]\n",
        "\n",
        "#                 resized_image = cv2.resize(plot_data_rgb, (224, 224))\n",
        "\n",
        "#                 if resized_image.shape[-1] == 1:\n",
        "#                    resized_image = cv2.cvtColor(resized_image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "\n",
        "\n",
        "#                 python_list = resized_image.tolist()\n",
        "\n",
        "#                 images.append(resized_image)\n",
        "#                 labels.append(label)\n",
        "\n",
        "#                 # image = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_height, image_width))\n",
        "#                 # image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "#                 # images.append(image)\n",
        "#                 # print(len(image),len(image[0]),len(image[0][0]))\n",
        "#                 # labels.append(label)\n",
        "#                 # # i=i+1\n",
        "#     return np.array(images), np.array(labels)\n",
        "\n",
        "# # Calculate the minimum number of samples available in either category for training\n",
        "# num_samples_d_train = len(os.listdir(os.path.join(train_dir, \"Test_D\")))\n",
        "# num_samples_nd_train = len(os.listdir(os.path.join(train_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_train = min(num_samples_d_train, num_samples_nd_train)\n",
        "\n",
        "# # Calculate the minimum number of samples available in either category for testing\n",
        "# num_samples_d_test = len(os.listdir(os.path.join(test_dir, \"Test_D\")))\n",
        "# num_samples_nd_test = len(os.listdir(os.path.join(test_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_test = min(num_samples_d_test, num_samples_nd_test)\n",
        "\n",
        "# train_images, train_labels = load_images_and_labels(train_dir, num_samples_per_category_train)\n",
        "# test_images, test_labels = load_images_and_labels(test_dir, num_samples_per_category_test)"
      ],
      "metadata": {
        "id": "cX-g0Ugm8J2l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wM3Y4mtEe6SA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from skimage import io, color, metrics, transform\n",
        "\n",
        "# Step 1: Define data directories\n",
        "train_dir = \"/content/gdrive/MyDrive/rnd1/week5/Training\"  # Contains \"utterances_d\" and \"utterances_nd\" folders\n",
        "test_dir = \"/content/gdrive/MyDrive/rnd1/week5/Test\"    # Contains \"test_d\" and \"test_nd\" folders\n",
        "\n",
        "\n",
        "image_height, image_width = 72,72  # Adjust as needed\n",
        "num_classes = 2  # Depressed and non-depressed\n",
        "\n",
        "def load_images_and_labels(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for label, category in enumerate([\"depressed\", \"non_depressed\"]):\n",
        "        category_dir = os.path.join(data_dir, category)\n",
        "        image_files = [f for f in os.listdir(category_dir) if f.endswith(\".png\")]\n",
        "\n",
        "        # If there are fewer samples in this category, select all available samples\n",
        "        # num_samples = min(len(image_files), num_samples_per_category)\n",
        "        # selected_files = np.random.choice(image_files, num_samples_per_category, replace=False)\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(category_dir, image_file)\n",
        "\n",
        "            image = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_height, image_width))\n",
        "            image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Calculate the minimum number of samples available in either category for training\n",
        "# num_samples_d_train = len(os.listdir(os.path.join(train_dir, \"Test_D\")))\n",
        "# num_samples_nd_train = len(os.listdir(os.path.join(train_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_train = min(num_samples_d_train, num_samples_nd_train)\n",
        "\n",
        "# Calculate the minimum number of samples available in either category for testing\n",
        "# num_samples_d_test = len(os.listdir(os.path.join(test_dir, \"Test_D\")))\n",
        "# num_samples_nd_test = len(os.listdir(os.path.join(test_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_test = min(num_samples_d_test, num_samples_nd_test)\n",
        "\n",
        "train_images, train_labels = load_images_and_labels(train_dir)\n",
        "test_images, test_labels = load_images_and_labels(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X1bclfwpD-ea"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from skimage import io, color, metrics, transform\n",
        "\n",
        "# # Step 1: Define data directories\n",
        "# train_dir = \"/content/gdrive/MyDrive/rnd/week5/Training\"  # Contains \"utterances_d\" and \"utterances_nd\" folders\n",
        "# test_dir = \"/content/gdrive/MyDrive/rnd/week5/Test\"     # Contains \"test_d\" and \"test_nd\" folders\n",
        "\n",
        "\n",
        "# image_height, image_width = 72,72  # Adjust as needed\n",
        "# num_classes = 2  # Depressed and non-depressed\n",
        "\n",
        "\n",
        "# def load_images_and_labels(data_dir, num_samples_per_category):\n",
        "#     # i=1\n",
        "#     images = []\n",
        "#     labels = []\n",
        "#     for label, category in enumerate([\"Test_D\", \"Test_ND\"]):\n",
        "#           category_dir = os.path.join(data_dir, category)\n",
        "#           image_files = [f for f in os.listdir(category_dir) if f.endswith(\".png\")]\n",
        "\n",
        "#           # If there are fewer samples in this category, select all available samples\n",
        "#           # num_samples = min(len(image_files), num_samples_per_category)\n",
        "#           selected_files = np.random.choice(image_files, num_samples_per_category, replace=False)\n",
        "\n",
        "#           for image_file in selected_files:\n",
        "#               # if i<3:\n",
        "#                 image_path = os.path.join(category_dir, image_file)\n",
        "\n",
        "#                 image = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_height, image_width))\n",
        "#                 image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "#                 images.append(image)\n",
        "#                 # print(len(image),len(image[0]),len(image[0][0]))\n",
        "#                 labels.append(label)\n",
        "#                 # i=i+1\n",
        "#     return np.array(images), np.array(labels)\n",
        "\n",
        "# # Calculate the minimum number of samples available in either category for training\n",
        "# num_samples_d_train = len(os.listdir(os.path.join(train_dir, \"Test_D\")))\n",
        "# num_samples_nd_train = len(os.listdir(os.path.join(train_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_train = min(num_samples_d_train, num_samples_nd_train)\n",
        "\n",
        "# # Calculate the minimum number of samples available in either category for testing\n",
        "# num_samples_d_test = len(os.listdir(os.path.join(test_dir, \"Test_D\")))\n",
        "# num_samples_nd_test = len(os.listdir(os.path.join(test_dir, \"Test_ND\")))\n",
        "# num_samples_per_category_test = min(num_samples_d_test, num_samples_nd_test)\n",
        "\n",
        "# train_images, train_labels = load_images_and_labels(train_dir, num_samples_per_category_train)\n",
        "# test_images, test_labels = load_images_and_labels(test_dir, num_samples_per_category_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OIj491UsfNc1"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1P6OoYeShLtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6c65af-9858-45bb-a0a2-01a1d2eb7a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "!pip install -U tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gbt9E_gxfQ6i"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.008\n",
        "weight_decay = 0.001\n",
        "batch_size = 256\n",
        "image_size = 72 # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]\n",
        "input_shape=(image_size,image_size ,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gLq__4UGfT6L"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        # layers.RandomFlip(\"horizontal\"),\n",
        "        # layers.RandomRotation(factor=0.02),\n",
        "        # layers.RandomZoom(\n",
        "            # height_factor=0.2, width_factor=0.2\n",
        "        # ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vw0TG1Nnfe4v"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6nupkkapfhad"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mfZa_weBfmJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "a704a32e-74fd-4dfa-ffa4-ed6ee91f0ac3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 144 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = train_images[np.random.choice(range(train_images.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fwqQeCxVfy9F"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0_JVKidGf2XP"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "\n",
        "    logits = layers.Dense(1, activation='sigmoid')(logits)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kFlHGurwf5T0"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.BinaryCrossentropy(name='accuracy1'),  # Update this line\n",
        "    metrics=[\n",
        "        keras.losses.BinaryCrossentropy(name='accuracy2'),\n",
        "        keras.losses.BinaryCrossentropy(name='accuracy231')\n",
        "    ],\n",
        ")\n",
        "\n",
        "    # checkpoint_filepath = load_path + \"vit/temp/checkpoint\"\n",
        "    # checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    #     checkpoint_filepath,\n",
        "    #     monitor=\"val_accuracy\",\n",
        "    #     save_best_only=True,\n",
        "    #     save_weights_only=True,\n",
        "    # )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=train_images,\n",
        "        y=train_labels,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.3\n",
        "        # callbacks=[checkpoint_callback],\n",
        "    )\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(test_images, test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "    # training_accuracy = history.history['accuracy']\n",
        "    # validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "    # # Create a parallel plot of training accuracy and validation accuracy\n",
        "    # plt.figure(figsize=(10, 5))\n",
        "    # plt.plot(range(1, num_epochs + 1), training_accuracy, label='Training Accuracy', marker='o')\n",
        "    # plt.plot(range(1, num_epochs + 1), validation_accuracy, label='Validation Accuracy', marker='x')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Accuracy')\n",
        "    # plt.title('Training vs. Validation Accuracy')\n",
        "    # plt.legend()\n",
        "    # plt.grid(True)\n",
        "    # plt.show()\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QIHld1gmgBx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20440ab6-c36b-4bff-c2ae-aff59a0b8703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 31s 31s/step - loss: 1.3326 - accuracy2: 1.3326 - accuracy231: 1.3326 - val_loss: 0.0000e+00 - val_accuracy2: 0.0000e+00 - val_accuracy231: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 381ms/step - loss: 275.2070 - accuracy2: 275.2070 - accuracy231: 275.2070 - val_loss: 0.0000e+00 - val_accuracy2: 0.0000e+00 - val_accuracy231: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 391.2681 - accuracy2: 391.2681 - accuracy231: 391.2681 - val_loss: 555.5659 - val_accuracy2: 555.5659 - val_accuracy231: 555.5659\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 359ms/step - loss: 390.7065 - accuracy2: 390.7065 - accuracy231: 390.7065 - val_loss: 1.7130e-21 - val_accuracy2: 1.7130e-21 - val_accuracy231: 1.7130e-21\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 362ms/step - loss: 12.3622 - accuracy2: 12.3622 - accuracy231: 12.3622 - val_loss: 1.1956e-38 - val_accuracy2: 1.1956e-38 - val_accuracy231: 1.1956e-38\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 28.6096 - accuracy2: 28.6096 - accuracy231: 28.6096 - val_loss: 0.1265 - val_accuracy2: 0.1265 - val_accuracy231: 0.1265\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 361ms/step - loss: 5.3878 - accuracy2: 5.3878 - accuracy231: 5.3878 - val_loss: 10.9413 - val_accuracy2: 10.9413 - val_accuracy231: 10.9413\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 339ms/step - loss: 6.3788 - accuracy2: 6.3788 - accuracy231: 6.3788 - val_loss: 0.7561 - val_accuracy2: 0.7561 - val_accuracy231: 0.7561\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 360ms/step - loss: 2.2885 - accuracy2: 2.2885 - accuracy231: 2.2885 - val_loss: 0.0021 - val_accuracy2: 0.0021 - val_accuracy231: 0.0021\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 352ms/step - loss: 2.1262 - accuracy2: 2.1262 - accuracy231: 2.1262 - val_loss: 0.0018 - val_accuracy2: 0.0018 - val_accuracy231: 0.0018\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.8798 - accuracy2: 0.8043 - accuracy231: 0.8043\n",
            "Test accuracy: 80.43%\n",
            "Test top 5 accuracy: 80.43%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "vit_classifier = create_vit_classifier()\n",
        "best_model = run_experiment(vit_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vi5sb9qBgCqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d42626-05b0-4553-8018-a9c7b57b1044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 2s 29ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9978161 ],\n",
              "       [0.99765635],\n",
              "       [0.9975783 ],\n",
              "       [0.9983606 ],\n",
              "       [0.99813026],\n",
              "       [0.99804306],\n",
              "       [0.9979602 ],\n",
              "       [0.9983315 ],\n",
              "       [0.997962  ],\n",
              "       [0.9977768 ],\n",
              "       [0.99836284],\n",
              "       [0.99798393],\n",
              "       [0.99825305],\n",
              "       [0.99860424],\n",
              "       [0.9979773 ],\n",
              "       [0.99786645],\n",
              "       [0.99785775],\n",
              "       [0.997785  ],\n",
              "       [0.9978275 ],\n",
              "       [0.998086  ],\n",
              "       [0.99823284],\n",
              "       [0.99783677],\n",
              "       [0.99772054],\n",
              "       [0.99798065],\n",
              "       [0.99830025],\n",
              "       [0.9979949 ],\n",
              "       [0.9978896 ],\n",
              "       [0.9977344 ],\n",
              "       [0.9980045 ],\n",
              "       [0.9979736 ],\n",
              "       [0.9981675 ],\n",
              "       [0.99836105],\n",
              "       [0.99786085],\n",
              "       [0.9977963 ],\n",
              "       [0.9980621 ],\n",
              "       [0.9979341 ],\n",
              "       [0.99864477],\n",
              "       [0.9978854 ],\n",
              "       [0.997825  ],\n",
              "       [0.9983354 ],\n",
              "       [0.9978922 ],\n",
              "       [0.9975647 ],\n",
              "       [0.9980228 ],\n",
              "       [0.99758613],\n",
              "       [0.9980586 ],\n",
              "       [0.9978752 ],\n",
              "       [0.9983084 ],\n",
              "       [0.9976713 ],\n",
              "       [0.99772614],\n",
              "       [0.99781895],\n",
              "       [0.9976598 ],\n",
              "       [0.99798906],\n",
              "       [0.99873906],\n",
              "       [0.99810886],\n",
              "       [0.998309  ],\n",
              "       [0.99796677],\n",
              "       [0.99796236],\n",
              "       [0.998125  ],\n",
              "       [0.99783295],\n",
              "       [0.99832636],\n",
              "       [0.9981943 ],\n",
              "       [0.99804616],\n",
              "       [0.9978388 ],\n",
              "       [0.99804175],\n",
              "       [0.99803144],\n",
              "       [0.9977335 ],\n",
              "       [0.9978231 ],\n",
              "       [0.99826956],\n",
              "       [0.99836665],\n",
              "       [0.99860007],\n",
              "       [0.99789774],\n",
              "       [0.9982405 ],\n",
              "       [0.9978536 ],\n",
              "       [0.9983499 ],\n",
              "       [0.9990515 ],\n",
              "       [0.99780124],\n",
              "       [0.9980171 ],\n",
              "       [0.9979819 ],\n",
              "       [0.9980147 ],\n",
              "       [0.9978002 ],\n",
              "       [0.99767345],\n",
              "       [0.9977876 ],\n",
              "       [0.99780756],\n",
              "       [0.99799675],\n",
              "       [0.99809533],\n",
              "       [0.9978661 ],\n",
              "       [0.99813867],\n",
              "       [0.99815625],\n",
              "       [0.9976739 ],\n",
              "       [0.99768543],\n",
              "       [0.9978599 ],\n",
              "       [0.998018  ],\n",
              "       [0.99796945],\n",
              "       [0.99760264],\n",
              "       [0.998429  ],\n",
              "       [0.9978775 ],\n",
              "       [0.99777263],\n",
              "       [0.9977411 ],\n",
              "       [0.9977937 ],\n",
              "       [0.9978624 ],\n",
              "       [0.9975273 ],\n",
              "       [0.9976545 ],\n",
              "       [0.99771935],\n",
              "       [0.99877995],\n",
              "       [0.9992181 ],\n",
              "       [0.99782985],\n",
              "       [0.9980604 ],\n",
              "       [0.9979259 ],\n",
              "       [0.99925786],\n",
              "       [0.99797016],\n",
              "       [0.9979971 ],\n",
              "       [0.99798524],\n",
              "       [0.99818367],\n",
              "       [0.99791306],\n",
              "       [0.9983999 ],\n",
              "       [0.9980856 ],\n",
              "       [0.9978417 ],\n",
              "       [0.9979907 ],\n",
              "       [0.99797374],\n",
              "       [0.9980444 ],\n",
              "       [0.9983266 ],\n",
              "       [0.99801576],\n",
              "       [0.9978428 ],\n",
              "       [0.99786025],\n",
              "       [0.99766624],\n",
              "       [0.9978194 ],\n",
              "       [0.99773264],\n",
              "       [0.9978377 ],\n",
              "       [0.9981066 ],\n",
              "       [0.9979267 ],\n",
              "       [0.9978719 ],\n",
              "       [0.9981893 ],\n",
              "       [0.9978849 ],\n",
              "       [0.9978725 ],\n",
              "       [0.998066  ],\n",
              "       [0.9976661 ],\n",
              "       [0.99752265],\n",
              "       [0.99791676],\n",
              "       [0.99811494],\n",
              "       [0.9979358 ],\n",
              "       [0.99775296],\n",
              "       [0.99775726],\n",
              "       [0.99818605],\n",
              "       [0.99776816],\n",
              "       [0.9982899 ],\n",
              "       [0.9980222 ],\n",
              "       [0.9980519 ],\n",
              "       [0.9976713 ],\n",
              "       [0.998184  ],\n",
              "       [0.9979365 ],\n",
              "       [0.99802446],\n",
              "       [0.99788266],\n",
              "       [0.9986651 ],\n",
              "       [0.998109  ],\n",
              "       [0.997886  ],\n",
              "       [0.9983059 ],\n",
              "       [0.9978728 ],\n",
              "       [0.99815863],\n",
              "       [0.9978325 ],\n",
              "       [0.9977543 ],\n",
              "       [0.99840003],\n",
              "       [0.99807125],\n",
              "       [0.99784005],\n",
              "       [0.9981237 ],\n",
              "       [0.9982241 ],\n",
              "       [0.9978611 ],\n",
              "       [0.9991492 ],\n",
              "       [0.9976636 ],\n",
              "       [0.9981895 ],\n",
              "       [0.9981517 ],\n",
              "       [0.9979755 ],\n",
              "       [0.9980996 ],\n",
              "       [0.9978224 ],\n",
              "       [0.9979095 ],\n",
              "       [0.9979195 ],\n",
              "       [0.99779654],\n",
              "       [0.99813914],\n",
              "       [0.99809664],\n",
              "       [0.9978587 ],\n",
              "       [0.99794585],\n",
              "       [0.99764997],\n",
              "       [0.9985784 ],\n",
              "       [0.9983359 ],\n",
              "       [0.99777204],\n",
              "       [0.9986707 ],\n",
              "       [0.99803156],\n",
              "       [0.9981172 ],\n",
              "       [0.99812704],\n",
              "       [0.9978072 ],\n",
              "       [0.9980806 ],\n",
              "       [0.99777204],\n",
              "       [0.9989311 ],\n",
              "       [0.99799794],\n",
              "       [0.9980197 ],\n",
              "       [0.9984754 ],\n",
              "       [0.9978811 ],\n",
              "       [0.998002  ],\n",
              "       [0.9980944 ],\n",
              "       [0.9979704 ],\n",
              "       [0.9980184 ],\n",
              "       [0.9976313 ],\n",
              "       [0.9980667 ],\n",
              "       [0.9980768 ],\n",
              "       [0.9979971 ],\n",
              "       [0.9982469 ],\n",
              "       [0.9980269 ],\n",
              "       [0.99795806],\n",
              "       [0.9983656 ],\n",
              "       [0.9978915 ],\n",
              "       [0.9975672 ],\n",
              "       [0.9976936 ],\n",
              "       [0.9976534 ],\n",
              "       [0.9979005 ],\n",
              "       [0.998252  ],\n",
              "       [0.9984572 ],\n",
              "       [0.9979456 ],\n",
              "       [0.9979461 ],\n",
              "       [0.99856275],\n",
              "       [0.99792   ],\n",
              "       [0.9978557 ],\n",
              "       [0.99786264],\n",
              "       [0.998142  ],\n",
              "       [0.99817944],\n",
              "       [0.997813  ],\n",
              "       [0.99795413],\n",
              "       [0.99789494],\n",
              "       [0.9986999 ],\n",
              "       [0.99813986],\n",
              "       [0.9978235 ],\n",
              "       [0.9984621 ],\n",
              "       [0.99834466],\n",
              "       [0.99776244],\n",
              "       [0.99818355],\n",
              "       [0.9980095 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "best_model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "76R45gkOgK2A"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plotConfusionMatrix(batch_labels,predicted_labels):\n",
        "\n",
        "    # Move the tensors to CPU memory\n",
        "    all_batch_labels_cpu = batch_labels\n",
        "    all_predicted_labels_cpu = predicted_labels\n",
        "\n",
        "    # Create the confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_batch_labels_cpu, all_predicted_labels_cpu)\n",
        "\n",
        "    # Plot the confusion matrix as a heatmap with a smaller figsize\n",
        "    plt.figure(figsize=(4, 4))  # Adjust the dimensions as needed\n",
        "    sns.set(font_scale=1.0)  # Adjust font scale as needed\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"viridis\", annot_kws={\"size\": 10}, cbar=False)\n",
        "\n",
        "    # Adjust tick positions and labels\n",
        "    tick_positions = [0.5, 1.5]  # Center of each cell\n",
        "    plt.gca().set_xticks(tick_positions)\n",
        "    plt.gca().set_yticks(tick_positions)\n",
        "    plt.gca().set_xticklabels([\"ND\", \"D\"])\n",
        "    plt.gca().set_yticklabels([\"ND\", \"D\"])\n",
        "\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7ZHncX4Oplwr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "9f91b25c-260f-47a6-c573-e310bd7e77d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Make predictions on the test dataset\n",
        "predictions = best_model.predict(test_images)\n",
        "\n",
        "binary_predictions_test = [np.argmax(x) for x in predictions]\n",
        "\n",
        "plotConfusionMatrix(test_labels,binary_predictions_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}